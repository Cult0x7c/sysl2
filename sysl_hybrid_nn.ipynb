{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HybridNetworks with Model Based Operators and Deep Learning\n",
    "\n",
    "When training neural networks, it is important to ensure that the distribution in the training data is similar to that of the test data. In this exercise we want to violate this assumption intentionally by using training and test data in which the illumination is different, but which otherwise come from the same distribution. We want to examine the generalization properties of the neural network. In addition, we want to incorporate model-based operators into our system to transform the data to an invariant space first, and then put it trough the neural architecture, attempting that the output of the transformation (input of the neural network) follows the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If training neural networks in pytorch is all new to you, this tutorial may be helpful:\n",
    " https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include dependencies\n",
    "import os  \n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    is_cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    is_cuda = False\n",
    "    \n",
    "print(device, is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "An image dataset of an simulated urban environment that consists of one large imageset, already split in a train and a validation set, where all images have the same illumination and a second small imageset with a different illumination.\n",
    "\n",
    "(In the training and evaluation of machine learning models the dataset is typically split into three sets.\n",
    "The training set is used to train the parameters of the model. The validation set is used to compare multiple hyperparametersettings (model-architecture, learningrate setting, ...). The test set is only used for the final evaluation of the model, where nothing is adjusted anymore, to achieve unbiased evaluation.)\n",
    "\n",
    "Here we will also have 3 sets, but with a different meaning. We don't want to tune hyperparameters but illustrate the generalization property of a model in multiple illumination scenarios, therefore we omit a testset here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "path = #TODO Specify path of 'urban_driving_cl' folder\n",
    "\n",
    "# TODO: load images as Datasets\n",
    "#  use torchvision.datasets.ImageFolder()\n",
    "#  specify a transform to convert Tensors to Tensors\n",
    "train_set = # TODO: Create dataset with images from os.path.join(path,\"driving_set1_cropped_train\")\n",
    "val_set1 = # TODO: Create dataset with images from os.path.join(path,\"driving_set1_cropped_val\")\n",
    "val_set2 = # TODO: Create dataset with images from os.path.join(path,\"driving_set2_cropped\")\n",
    "\n",
    "\n",
    "# TODO: Create the dataloaders that sample from corresponding dataset\n",
    "#  use torch.utils.data.DataLoader()\n",
    "#  set batch size and shuffle(==True for the trainset, and alo for \n",
    "#  the ohers if you want to see a good mix of all classes)\n",
    "batch_size = 32\n",
    "dataloader_train = # TODO\n",
    "dataloader_val1 =  # TODO\n",
    "dataloader_val2 =  # TODO\n",
    "\n",
    "\n",
    "#vizualize the sets\n",
    "for dl in [dataloader_train, dataloader_val1, dataloader_val2]:\n",
    "    #And vizualize\n",
    "    grid_img = torchvision.utils.make_grid(next(iter(dl))[0], nrow=8)\n",
    "    \n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "First we build our Convolutional Neural Network model.\n",
    "\n",
    "**You can define your own model**, or use the one described below. The input is expected to have a witdth, height of 64x64, while the number of channels should be a parameter (as the transforms have different channel output)\n",
    "\n",
    "#### Possible Model Architecture:\n",
    "* 3 Convolution Layers (nn.Conv2d) each with 64 kernels,\n",
    "    - activated with ReLU (F.relu) \n",
    "    - followed by a 2x2 MaxPool (nn.MaxPool2d)\n",
    "        - 1. 5x5 kernels -> output-shape (after maxpool) (batch_size, 64, 30x30)\n",
    "        - 2. 3x3 kernels -> output-shape (after maxpool) (batch_size, 64, 14x14)\n",
    "        - 3. 3x3 kernels -> output-shape (after maxpool) (batch_size, 64, 6x6)\n",
    "* 2 Fully Connected Layers (nn.Linear), with 100 hidden layers:\n",
    "    - 32x6x6 input units to 100 hidden units \n",
    "    - 100 hidden units to num_classes\n",
    "    - the first fc is activated wit ReLU\n",
    "    - the second fc must not be activated as pytorch auomatically applies a softmax in the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DEFINE THE CNN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,in_c = 3, num_classes = 5):\n",
    "        super().__init__()\n",
    "        #expected input: b,c,64, 64\n",
    "        #TODO\n",
    "  \n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)\n",
    "print(net)\n",
    "#check if runs and has the correct output:\n",
    "print(net(torch.zeros((16,3,64,64)).to(device)).shape, \"== [16,5]?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop\n",
    "Create a function for training a model and to evaluate its performance.\n",
    "#### Training:\n",
    "* Loop over the dataset\n",
    "* Feed the inputs to the network\n",
    "* Calculate the Loss with a criterion\n",
    "* Run optimizer on parameters\n",
    "\n",
    "#### Evaluation:\n",
    "Calculate the accurracy of the model by counting the number of true predictions(the class of the image has been predicted correctly i.e. has the maximum weight).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train(True)\n",
    "    # get the inputs and targets; data is a list of [inputs, targets]\n",
    "    for i, (inputs,targets) in enumerate(dataloader, 0):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "def eval(model,dataloader):\n",
    "    model.train(False)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        # TODO get the inputs and targets; data is a list of [inputs, targets]\n",
    "        for ...\n",
    "            # TODO send inputs and targets to device\n",
    "            \n",
    "            # calculate outputs by running images through the network\n",
    "            # TODO\n",
    "            \n",
    "            # the class with the highest weight is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    return correct/total\n",
    " \n",
    "net = Net().to(device)\n",
    "\n",
    "#citerion: the function that defines how the ouptut and desired target are compared\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "#optimizer: an iterative method for optimizing the parameters based on the computed gradient\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) \n",
    "\n",
    "# Check if model learns:\n",
    "# The accurracy should go above 90%\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs): # loop over the dataset multiple times\n",
    "    print(\"epoch %i /%i\"%(epoch+1, num_epochs))\n",
    "    train(net, dataloader_train, criterion,optimizer)\n",
    "    print(\"Accuracy on set1 (validation set) is %f %%\"%(eval(net, dataloader_val1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "In the lecture the rg transform an lbp transform was presented, that should be implemented and combined with the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Implememtation of a transformation, \n",
    "# calculating the intensity of a batch of pytorch images\n",
    "\n",
    "class Intensity(object):\n",
    "    \"\"\" transform RGB image to Intensity Image\"\"\"\n",
    "    def __call__(self, x):\n",
    "        intensity = x.sum(dim =1)\n",
    "        intensity = intensity.unsqueeze(1)/3.0\n",
    "        return intensity\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Intensity()\"\n",
    "    \n",
    "intensity_transform = Intensity()\n",
    "#And vizualize\n",
    "images = next(iter(dataloader_train))[0].to(device)\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=8)\n",
    "grid_img_t = torchvision.utils.make_grid(intensity_transform(images), nrow=8)\n",
    "plt.imshow(grid_img.permute(1, 2, 0).cpu().detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(grid_img_t.permute(1, 2, 0).cpu().detach().numpy())\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rg Transform\n",
    "$r= \\frac{R}{R+G+B}$ $g= \\frac{G}{R+G+B}$\n",
    "\n",
    "This model is invariant to shadow, light intensity and light direction, but has problems with highlight under the assumption of neutral i.e. white light, and assuming Shaferâ€™s dichromatic reflection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rg\n",
    "class NormalizedRG(object):\n",
    "    \"\"\" transform RGB image to normalized RG\"\"\"\n",
    "    def __call__(self, x):\n",
    "        eps = 1e-7\n",
    "        x = (x+eps)\n",
    "        rgb = #TODO\n",
    "        return rgb[:,0:2,:,:]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"NormalizedRG()\"\n",
    "    \n",
    "#And vizualize\n",
    "rg_transform = NormalizedRG()\n",
    "images = next(iter(dataloader_train))[0].to(device)\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=8)\n",
    "rg_images = rg_transform(images)\n",
    "grid_img_r = torchvision.utils.make_grid([rg[0].unsqueeze(dim =0) for rg in rg_images], nrow=8)\n",
    "grid_img_g = torchvision.utils.make_grid([rg[1].unsqueeze(dim =0) for rg in rg_images], nrow=8)\n",
    "print(\"RGB\")\n",
    "plt.imshow(grid_img.permute(1, 2, 0).cpu().detach().numpy())\n",
    "plt.show()\n",
    "print(\"r\")\n",
    "plt.imshow(grid_img_r.permute(1, 2, 0).cpu().detach().numpy())\n",
    "plt.show()\n",
    "print(\"g\")\n",
    "plt.imshow(grid_img_g.permute(1, 2, 0).cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LBP Transformation\n",
    "* Works on a grayscale image/ intensity of the pixels\n",
    "* Create p neighbouring positions in r distance to center pixel\n",
    "* Threshholding neighbouring pixel with center pixel\n",
    "    * If neighbour >= center pixel => 1 else 0\n",
    "    * Create decimal number from binary result\n",
    " \n",
    "* Implemented using Conv2d convolutions to use the GPU implementation\n",
    "\n",
    "This model is invariant to monotonic illumination changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBP(object):\n",
    "    def __init__(self,radius=1, points=8):\n",
    "        self.radius = radius\n",
    "        self.points = points\n",
    "        \n",
    "        #FILTER TO CONVERT TO GRAYSCALE\n",
    "        self.intensity = Intensity()\n",
    "        \n",
    "        #FILTER TO COMPARE WITH NEIGHBOURS\n",
    "        self.size = self.radius*2+1\n",
    "        self.neighbor_positions = self.positions()\n",
    "        self.lbp_conv1 = nn.Conv2d(in_channels = 1, out_channels = self.points, kernel_size=self.size, padding =self.radius, bias = False)\n",
    "        self.lbp_conv1.weight.data.fill_(0.0)\n",
    "        for i, (w, h) in enumerate(self.neighbor_positions):\n",
    "            self.lbp_conv1.weight.data[i,0,self.size//2,self.size//2]  = -1\n",
    "            self.lbp_conv1.weight.data[i,0,w,h]= 1\n",
    "        #print(self.lbp_conv1.weight.data)\n",
    "        self.lbp_conv1.to(device)\n",
    "\n",
    "        #FILTER TO CONVERT BINARY NUMBER(0,1 in channels) TO DECIMAL\n",
    "        self.lbp_conv2 = nn.Conv2d(in_channels = self.points, out_channels = 1, kernel_size=1,bias = False)\n",
    "        for i,i_chan in enumerate(range(self.points)):\n",
    "            self.lbp_conv2.weight.data[0,i,0,0] = 2**i\n",
    "        #print(self.lbp_conv2.weight.data)\n",
    "        self.lbp_conv2.to(device)\n",
    "        self.max_value = float(2**(points+1)-1) #max decimal number, defined by number of points\n",
    "\n",
    "    def positions(self):\n",
    "        mid  = self.radius\n",
    "        positions =[]\n",
    "        for i in range(self.points):\n",
    "            #calculate angle and according position for every point\n",
    "            alpha = 2*math.pi / self.points *i \n",
    "            x = int(round(mid + self.radius *math.cos(alpha)))\n",
    "            y = int(round(mid + self.radius *math.sin(alpha)))\n",
    "            positions.append((x,y))\n",
    "        #print(positions)\n",
    "        return positions\n",
    "\n",
    "    def __call__(self,x):\n",
    "        # convert to grayscale(using the intensity tranformation)\n",
    "        x = #TODO\n",
    "        \n",
    "        # compare the the neighbouring pixels to that of the central pixel\n",
    "        #  applying lbp_conv1 \n",
    "        #  (to create for every pixel for each neighbour point the difference of the point to the middle)\n",
    "        x = #TODO \n",
    "        \n",
    "        # convert to binary: 0 if less 0\n",
    "        x[x >= 0] = #TODO\n",
    "        x[x < 0] = #TODO\n",
    "        \n",
    "        # convert to decimal\n",
    "        #  apply lbp_conv2\n",
    "        x = #TODO\n",
    "\n",
    "        x= x/self.max_value\n",
    "        return x\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"LBP(radius_%i_points_%i)\"%(self.radius, self.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbp = LBP(1,8)\n",
    "#And vizualize\n",
    "images = next(iter(dataloader_train))[0].to(device)\n",
    "grid_img = torchvision.utils.make_grid(images, nrow=8)\n",
    "grid_img_t = torchvision.utils.make_grid(lbp(images), nrow=8)\n",
    "plt.imshow(grid_img.permute(1, 2, 0).cpu().detach().numpy())\n",
    "plt.show()\n",
    "plt.imshow(grid_img_t.permute(1, 2, 0).cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Net\n",
    "A class combining the model based transform and the CNN\n",
    "* The model based transform is applied with disabled gradient calculation as the weights are(/should be) fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, transform, nn):\n",
    "        super(HybridNet, self).__init__()\n",
    "        self.transform = transform\n",
    "        self.nn = nn\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad() :\n",
    "            x = #TODO apply transform\n",
    "        x = #TOOD apply nn\n",
    "        return x\n",
    "\n",
    "    def __str__(self):\n",
    "        info =\"(model_based_transform): %s \\n\"%(str(self.transform))\n",
    "        info += \"(CNN): %s\\n\"%(str(self.nn))\n",
    "        return info\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the experiements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run(model, num_epochs=20):\n",
    "    print(\"Train and evaluate model %s\"%(str(model)))\n",
    "    accurracys=[]\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    acc_val1 = eval(model, dataloader_val1)*100\n",
    "    acc_val2 = eval(model, dataloader_val2)*100\n",
    "    accurracys.append((acc_val1,acc_val2))\n",
    "\n",
    "    print(\"epoch 0 /%i Acc(set1): %f %%, Acc(set2): %f %%\"%( num_epochs, acc_val1, acc_val2))\n",
    "    for epoch in range(num_epochs): # loop over the dataset multiple times\n",
    "        #train model\n",
    "        #TODO\n",
    "        \n",
    "        #calculatte accurracys\n",
    "        acc_val1 = #TODO\n",
    "        acc_val2 = #TODO\n",
    "        accurracys.append((acc_val1,acc_val2))\n",
    "        print(\"epoch %i /%i Acc(set1): %f %%, Acc(set2): %f %%\"%(epoch+1, num_epochs, acc_val1, acc_val2))\n",
    "    #plot accurracys\n",
    "    plt.plot(list(range(num_epochs+1)), [a[0] for a in accurracys])\n",
    "    plt.plot(list(range(num_epochs+1)), [a[1] for a in accurracys])\n",
    "    plt.ylim([0, 100])\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = #TODO\n",
    "run(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid (NormalizedRG and CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Hybrid of the normalized RG and CNN Model\n",
    "rg_cnn_model = #TODO\n",
    "run(rg_cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid (LBP and CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Hybrid of LBP(1,8) and the CNN Model\n",
    "lbp_cnn_model = #TODO\n",
    "run(lbp_cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "**TODO:** Write down your conclusion from the experiements in 2-4 sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
